{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate coefficient of variation as per Gl端er et al. (1995)\n",
    "\n",
    "Author: Simone Poncioni, MSB, ARTORG Center for Biomedical Engineering Research, University of Bern, Switzerland\n",
    "\n",
    "Date: 07.2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cv_gluer(infile: pd.DataFrame, outfile: dict, variable: str):\n",
    "    \"\"\"Function to calculate the coefficient of variation as per Gl端er et al. 1995\n",
    "\n",
    "    Args:\n",
    "        infile (pd.DataFrame): DataFrame containing the imported yaml file with the simulation grouping information\n",
    "        outfile (dict): simulation results file\n",
    "        variable (str): variable to analyse (e.g. 'Stiffness', 'yield_force', etc.)\n",
    "    \"\"\"\n",
    "    def dofs(results):\n",
    "        '''\n",
    "        Equation (7), calculating the degrees of fredom df_j of the measurements in the individuals\n",
    "        '''\n",
    "        counter = Counter()\n",
    "        for value in results.values():\n",
    "            key_part = value[0].split('/')[1]\n",
    "            counter[key_part] += 1\n",
    "        degrees_of_freedom = sum(count - 1 for count in counter.values())\n",
    "        return degrees_of_freedom\n",
    "\n",
    "    def precision_error(results):\n",
    "        '''\n",
    "        Equation (6), calculating the generic standard deviation sd of the measurements\n",
    "        '''\n",
    "        values_sorted = defaultdict(list)\n",
    "        for _, value in results.items():\n",
    "            key_part = value[0].split('/')[1]\n",
    "            values_sorted[key_part].append(value[1])\n",
    "        \n",
    "        sum_tot = 0\n",
    "        total_elements = 0\n",
    "        df = 0\n",
    "\n",
    "        # Calculating the internal mean, sum of squared differences, and degrees of freedom\n",
    "        for key, values in values_sorted.items():\n",
    "            internal_mean = sum(values) / len(values)\n",
    "            total_elements += len(values)\n",
    "            df += len(values) - 1  # Adding to degrees of freedom\n",
    "            \n",
    "            for xij in values:\n",
    "                sum_tot += (xij - internal_mean) ** 2\n",
    "\n",
    "        # Dividing by degrees of freedom\n",
    "        variance = sum_tot / df\n",
    "        sd = variance ** 0.5\n",
    "\n",
    "        return sd, values_sorted\n",
    "\n",
    "    def coefficient_of_variation(sd, values_sorted):\n",
    "        '''\n",
    "        Equation (5), calculating the coefficient of variation on a percentage basis\n",
    "        '''\n",
    "        HUNDRED = 100\n",
    "        sum_means = 0\n",
    "        m = len(values_sorted)\n",
    "\n",
    "        for _, values in values_sorted.items():\n",
    "            internal_mean = sum(values) / len(values)\n",
    "            sum_means += internal_mean\n",
    "        \n",
    "        mean_of_means = sum_means / m\n",
    "        cv_sd = (sd / mean_of_means) * HUNDRED\n",
    "        return cv_sd\n",
    "\n",
    "    out_samples = outfile['Sample'].values\n",
    "    out_samples = [str(sample) for sample in out_samples]\n",
    "    results_dict = {}\n",
    "    for sample in out_samples:\n",
    "        variable_res = outfile[outfile['Sample'] == sample][str(variable)]\n",
    "        # add key 'variable' to results_dict (which has sample as key, infile['simulations']['folder_id'][sample] as value 0, and 'variable' as value 1)\n",
    "        results_dict[sample] = [infile['simulations']['folder_id'][sample], variable_res.values[0]]\n",
    "\n",
    "    # split results_dict into two dictionaries: results_r and results_t\n",
    "    # ->>> results_r contains infile['simulations']['folder_id'][sample][-1] == 'R'\n",
    "    # ->>> results_t contains infile['simulations']['folder_id'][sample][-1] == 'T'\n",
    "\n",
    "    results_r = {}\n",
    "    results_t = {}\n",
    "    for key, value in results_dict.items():\n",
    "        if value[0][-1] == 'R':\n",
    "            results_r[key] = value\n",
    "        elif value[0][-1] == 'T':\n",
    "            results_t[key] = value\n",
    "            \n",
    "    property_r = [value[1] for value in results_r.values()]\n",
    "    property_t = [value[1] for value in results_t.values()]\n",
    "\n",
    "    property = [property_r, property_t]\n",
    "    results = [results_r, results_t]\n",
    "    site = ['RADIUS', 'TIBIA']\n",
    "\n",
    "    df1 = pd.DataFrame()\n",
    "    # as per Gl端er et al. 1995\n",
    "    for res, stiff, _site in zip(results, property, site):\n",
    "        m = len(stiff)\n",
    "        df = dofs(res)\n",
    "        sd, values_sorted = precision_error(res)\n",
    "        cv_sd = coefficient_of_variation(sd, values_sorted)\n",
    "        # append to df\n",
    "        df_temp = pd.DataFrame({'Variable': variable, 'm': m, 'df': df, 'sd': sd, 'cv_sd': cv_sd}, index=[_site])\n",
    "        df1 = pd.concat([df1, df_temp])\n",
    "\n",
    "    df1.index.name = 'Site'\n",
    "    df1.reset_index(inplace=True)\n",
    "    df1.set_index(['Site', 'Variable'], inplace=True)\n",
    "    df1.reset_index(inplace=True)  # Reset index to make 'Site' and 'Variable' regular columns\n",
    "\n",
    "    return df1\n",
    "\n",
    "\n",
    "def pivot_dataframe(df: pd.DataFrame):\n",
    "    df_res_pivot = df.pivot(index='Variable', columns='Site')\n",
    "    df_res_pivot.columns = ['_'.join(col).strip() for col in df_res_pivot.columns.values]\n",
    "    new_column_order = ['m_RADIUS', 'df_RADIUS', 'sd_RADIUS', 'cv_sd_RADIUS', 'm_TIBIA', 'df_TIBIA', 'sd_TIBIA', 'cv_sd_TIBIA']\n",
    "    df_res_pivot = df_res_pivot[new_column_order]\n",
    "    return df_res_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the results\n",
    "in_path = Path('summaries/simulations-repro.yaml')\n",
    "with open(in_path, 'r') as f:\n",
    "    infile = yaml.safe_load(f)\n",
    "\n",
    "# * Indermaur et al. (in preparation, 2024)\n",
    "# outfile_path = Path('summaries/REPRO_mech_param_FEA_noReg__V_00_noReg_FZ_MAX_sphere_changed.csv')\n",
    "# variables_to_analyse = ['Stiffness', 'yield_force']\n",
    "\n",
    "# * Ours (Poncioni et al., in preparation, 2024)\n",
    "outfile_path = Path('summaries/06_repro_paper_data_summary.csv')\n",
    "variables_to_analyse = ['stiffness_1D_FZ_MAX', 'yield_force_FZ_MAX', 'SDV_BVTVT_Centroid', 'SDV_BVTVC_Centroid']\n",
    "outfile = pd.read_csv(outfile_path, delimiter=',')\n",
    "rho_file = pd.read_csv('summaries/rho.csv', delimiter=',')\n",
    "outfile = pd.merge(outfile, rho_file, on='Sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     m_RADIUS  df_RADIUS    sd_RADIUS  cv_sd_RADIUS  m_TIBIA  df_TIBIA     sd_TIBIA  cv_sd_TIBIA\n",
      "Variable                                                                                                        \n",
      "stiffness_1D_FZ_MAX        94         61  2476.682480      4.954443      109        70  2016.828952     2.312952\n",
      "yield_force_FZ_MAX         94         61   476.562436      6.200679      109        70   516.608358     2.904229\n",
      "SDV_BVTVT_Centroid         94         61     0.002032      1.164747      109        70     0.001914     0.916255\n",
      "SDV_BVTVC_Centroid         94         61     0.001919      2.121583      109        70     0.001518     1.716370\n"
     ]
    }
   ],
   "source": [
    "# Analyze the results as per Gl端er et al. 1995\n",
    "df_results = pd.DataFrame()\n",
    "\n",
    "for var in variables_to_analyse:\n",
    "    df_res = calculate_cv_gluer(infile, outfile, var)\n",
    "    df_res_pivot = pivot_dataframe(df_res)\n",
    "    df_results = pd.concat([df_results, df_res_pivot])\n",
    "\n",
    "print(df_results.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                m_RADIUS  df_RADIUS    sd_RADIUS  cv_sd_RADIUS  m_TIBIA  df_TIBIA     sd_TIBIA  cv_sd_TIBIA\n",
      "Variable                                                                                                   \n",
      "Stiffness             95         62  1099.772061      2.542580      113        74  1164.026018     1.516933\n",
      "yield_force           95         62   262.802508      4.041041      113        74   353.107813     2.314418\n",
      "yield_disp            95         62     0.004089      2.143722      113        74     0.002858     1.099886\n",
      "max_force             95         62   398.499775      4.947241      113        74   380.340335     2.256042\n",
      "disp_max_force        95         62     0.000000      0.000000      113        74     0.000000     0.000000\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'summaries/03_repro_vtu_data_summary_with_volumes.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m df_results \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m outfile, variables_to_analyse \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(outfiles, variables_to_analyse_list):\n\u001b[0;32m---> 16\u001b[0m     outfile_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m variables_to_analyse:\n\u001b[1;32m     18\u001b[0m         df_res \u001b[38;5;241m=\u001b[39m calculate_cv_gluer(infile, outfile_df, var)\n",
      "File \u001b[0;32m~/anaconda3/envs/hfe-accurate/lib/python3.9/site-packages/pandas-2.0.2-py3.9-linux-x86_64.egg/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/hfe-accurate/lib/python3.9/site-packages/pandas-2.0.2-py3.9-linux-x86_64.egg/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/envs/hfe-accurate/lib/python3.9/site-packages/pandas-2.0.2-py3.9-linux-x86_64.egg/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/hfe-accurate/lib/python3.9/site-packages/pandas-2.0.2-py3.9-linux-x86_64.egg/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/envs/hfe-accurate/lib/python3.9/site-packages/pandas-2.0.2-py3.9-linux-x86_64.egg/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'summaries/03_repro_vtu_data_summary_with_volumes.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the results\n",
    "in_path = Path('summaries/simulations-repro.yaml')\n",
    "with open(in_path, 'r') as f:\n",
    "    infile = yaml.safe_load(f)\n",
    "\n",
    "\n",
    "outfiles = [Path('summaries/REPRO_mech_param_FEA_noReg__V_00_noReg_FZ_MAX_sphere_changed.csv'), Path('summaries/03_repro_vtu_data_summary_with_volumes.csv')]\n",
    "variables_to_analyse_list = [['Stiffness', 'yield_force', 'yield_disp', 'max_force', 'disp_max_force'], ['stiffness_1D_FZ_MAX', 'yield_force_FZ_MAX', 'yield_disp_FZ_MAX', 'max_force_FZ_MAX', 'disp_at_max_force_FZ_MAX']]\n",
    "\n",
    "df_results = pd.DataFrame()\n",
    "for outfile, variables_to_analyse in zip(outfiles, variables_to_analyse_list):\n",
    "    \n",
    "    outfile_df = pd.read_csv(outfile, delimiter=',')\n",
    "    for var in variables_to_analyse:\n",
    "        df_res = calculate_cv_gluer(infile, outfile_df, var)\n",
    "        # Pivot the DataFrame\n",
    "        df_res_pivot = pivot_dataframe(df_res)\n",
    "        \n",
    "        df_results = pd.concat([df_results, df_res_pivot])\n",
    "    print(df_results.to_string())\n",
    "    if outfile == outfiles[0]:\n",
    "        df1 = df_results.copy()\n",
    "    else:\n",
    "        df2 = df_results.copy()\n",
    "\n",
    "# df_tot = pd.concat([df1, df2], axis=0)\n",
    "df_tot = df2\n",
    "# Assuming df1 and df2 are already loaded\n",
    "\n",
    "# Normalize column names for comparison\n",
    "# Map df1 columns to df2 columns based on the order specified in the question\n",
    "columns_mapping = {\n",
    "    'Stiffness': 'stiffness_1D_FZ_MAX',\n",
    "    'yield_force': 'yield_force_FZ_MAX',\n",
    "    'yield_disp': 'yield_disp_FZ_MAX',\n",
    "    'max_force': 'max_force_FZ_MAX',\n",
    "    'disp_max_force': 'disp_at_max_force_FZ_MAX'\n",
    "}\n",
    "\n",
    "# Prepare comparison data\n",
    "comparison_data = {\n",
    "    'Variable': [],\n",
    "    'cv_sd_RADIUS_df1': [],\n",
    "    'cv_sd_TIBIA_df1': [],\n",
    "    'cv_sd_RADIUS_df2': [],\n",
    "    'cv_sd_TIBIA_df2': []\n",
    "}\n",
    "\n",
    "print(f'Complete data:\\n\\n{df_tot.to_string()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('/home/simoneponcioni/Documents/00_GENERAL/pos_monitor.mplstyle')\n",
    "\n",
    "# get index of the dataframe\n",
    "index = df_tot.index\n",
    "\n",
    "# Mapping of properties\n",
    "columns_mapping = {\n",
    "    'Stiffness': 'stiffness_1D_FZ_MAX',\n",
    "    'yield_force': 'yield_force_FZ_MAX',\n",
    "    'yield_disp': 'yield_disp_FZ_MAX',\n",
    "    'max_force': 'max_force_FZ_MAX',\n",
    "    'disp_max_force': 'disp_at_max_force_FZ_MAX'\n",
    "}\n",
    "\n",
    "# Create a new DataFrame with both original and mapped values\n",
    "properties = list(columns_mapping.keys())\n",
    "mapped_properties = list(columns_mapping.values())\n",
    "\n",
    "# Extract cv_sd_RADIUS values\n",
    "original_values = df_tot.loc[properties, 'cv_sd_RADIUS']\n",
    "mapped_values = df_tot.loc[mapped_properties, 'cv_sd_RADIUS']\n",
    "\n",
    "# Create a new DataFrame to store the results\n",
    "plot_data = pd.DataFrame({\n",
    "    'Property': properties + mapped_properties,\n",
    "    'cv_sd_RADIUS': list(original_values) + list(mapped_values)\n",
    "})\n",
    "\n",
    "# Create a new DataFrame with both original and mapped values\n",
    "plot_data = []\n",
    "for orig, mapped in columns_mapping.items():\n",
    "    plot_data.append({\n",
    "        'Property': orig,\n",
    "        'cv_sd_RADIUS': abs(df_tot.loc[orig, 'cv_sd_RADIUS']),\n",
    "        'Type': 'Original'\n",
    "    })\n",
    "    plot_data.append({\n",
    "        'Property': mapped,\n",
    "        'cv_sd_RADIUS': abs(df_tot.loc[mapped, 'cv_sd_RADIUS']),\n",
    "        'Type': 'Mapped'\n",
    "    })\n",
    "\n",
    "plot_data = pd.DataFrame(plot_data)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "bar_width = 0.5\n",
    "indices = np.arange(len(columns_mapping))\n",
    "positions = np.array([2 * i for i in range(len(columns_mapping))])\n",
    "\n",
    "original_bars = plt.bar(positions, plot_data[plot_data['Type'] == 'Original']['cv_sd_RADIUS'], bar_width, label='Indermaur et al. (2021)')\n",
    "spline_bars = plt.bar(positions + bar_width, plot_data[plot_data['Type'] == 'Mapped']['cv_sd_RADIUS'], bar_width, label='Ours')\n",
    "\n",
    "# Add labels\n",
    "plt.ylabel('$CV_{SD}$ Radius')\n",
    "plt.title('Comparison of CV - Radius', weight='bold')\n",
    "middle_positions = positions + bar_width // 2\n",
    "plt.xticks(ticks=middle_positions, labels=columns_mapping.keys(), rotation=45)\n",
    "plt.legend()\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get index of the dataframe\n",
    "index = df_tot.index\n",
    "\n",
    "# Mapping of properties\n",
    "columns_mapping = {\n",
    "    'Stiffness': 'stiffness_1D_FZ_MAX',\n",
    "    'yield_force': 'yield_force_FZ_MAX',\n",
    "    'yield_disp': 'yield_disp_FZ_MAX',\n",
    "    'max_force': 'max_force_FZ_MAX',\n",
    "    'disp_max_force': 'disp_at_max_force_FZ_MAX'\n",
    "}\n",
    "\n",
    "# Create a new DataFrame with both original and mapped values\n",
    "properties = list(columns_mapping.keys())\n",
    "mapped_properties = list(columns_mapping.values())\n",
    "\n",
    "# Extract cv_sd_TIBIA values\n",
    "original_values = df_tot.loc[properties, 'cv_sd_TIBIA']\n",
    "mapped_values = df_tot.loc[mapped_properties, 'cv_sd_TIBIA']\n",
    "\n",
    "# Create a new DataFrame to store the results\n",
    "plot_data = pd.DataFrame({\n",
    "    'Property': properties + mapped_properties,\n",
    "    'cv_sd_TIBIA': list(original_values) + list(mapped_values)\n",
    "})\n",
    "\n",
    "# Create a new DataFrame with both original and mapped values\n",
    "plot_data = []\n",
    "for orig, mapped in columns_mapping.items():\n",
    "    plot_data.append({\n",
    "        'Property': orig,\n",
    "        'cv_sd_TIBIA': abs(df_tot.loc[orig, 'cv_sd_TIBIA']),\n",
    "        'Type': 'Original'\n",
    "    })\n",
    "    plot_data.append({\n",
    "        'Property': mapped,\n",
    "        'cv_sd_TIBIA': abs(df_tot.loc[mapped, 'cv_sd_TIBIA']),\n",
    "        'Type': 'Mapped'\n",
    "    })\n",
    "\n",
    "plot_data = pd.DataFrame(plot_data)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "bar_width = 0.5\n",
    "indices = np.arange(len(columns_mapping))\n",
    "positions = np.array([2 * i for i in range(len(columns_mapping))])\n",
    "\n",
    "original_bars = plt.bar(positions, plot_data[plot_data['Type'] == 'Original']['cv_sd_TIBIA'], bar_width, label='Indermaur et al. (2021)')\n",
    "spline_bars = plt.bar(positions + bar_width, plot_data[plot_data['Type'] == 'Mapped']['cv_sd_TIBIA'], bar_width, label='Ours')\n",
    "\n",
    "# Add labels\n",
    "plt.ylabel('$CV_{SD}$ Tibia')\n",
    "plt.title('Comparison of CV - Tibia', weight='bold')\n",
    "middle_positions = positions + bar_width // 2\n",
    "plt.xticks(ticks=middle_positions, labels=columns_mapping.keys(), rotation=45)\n",
    "plt.legend()\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
